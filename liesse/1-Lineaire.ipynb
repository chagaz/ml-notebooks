{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage LIESSE – TP 1 – Régressions linéaire et logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce notebook est de présenter la régression linéaire et la régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numpy, matplotlib et pandas__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scikit-learn__\n",
    "\n",
    "[Scikit-learn](http://scikit-learn.org) est le module Python par excellence pour les algorithmes de machine learning classiques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler sur le Problème 1 défini dans le notebook `0-Donnees.ipynb`. Chargeons de nouveau les données, de la même manière que dans ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = penguins[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]].to_numpy()\n",
    "y_regress = penguins[\"body_mass_g\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalisme \n",
    "\n",
    "Nous disposons de $n$ observations en $p$ dimensions, représentés par une matrice $X \\in \\mathbb{R}^{nxp}$ (ici, $p=3$), et d'un vecteur d'étiquettes $\\boldsymbol{y} \\in \\mathbb{R}^{n}$. Le but d'une régression linéaire est d'apprendre une fonction linéaire,\n",
    "\n",
    "$$f: \\boldsymbol{x} \\in \\mathbb{R}^p \\mapsto \\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x},$$\n",
    "\n",
    "où $\\beta_0 \\in \\mathbb{R}$ et $\\boldsymbol\\beta \\in \\mathbb{R}^p$.\n",
    "\n",
    "Pour ce faire, on __minimise le risque empirique__, calculé avec l'__erreur quadratique__ $(y, f(\\boldsymbol{x})) \\mapsto \\left(y - f(\\boldsymbol{x}) \\right)^2 $ comme fonction de perte. \n",
    "\n",
    "Il s'agit donc de trouver $\\beta_0, \\boldsymbol{\\beta}$ qui minimisent\n",
    "$$ J(\\beta_0, \\boldsymbol{\\beta}) = \\frac1n \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x}_i))\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérons les valeurs de $n$ (`n_samples`) et $p$ (`n_features`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nous avons n=%d observations et p=%d variables.\" % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire avec scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser `scikit-learn` pour trouver $\\beta_0, \\boldsymbol{\\beta}$ qui minimisent $J$.\n",
    "\n",
    "En machine learning, on parle d'__apprendre__ le modèle $f$.\n",
    "\n",
    "Les modèles linéaires sont implémentés dans le module [`linear_model`](https://scikit-learn.org/stable/modules/linear_model.html). La régression linéaire elle-même est implémentée dans la classe `linear_model.LinearRegression`. Vous pouvez consulter la documentation de cette classe [ici](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entraînement d'un modèle d'apprentissage suit toujours la même logique dans `scikit-learn` : \n",
    "1. On instancie un objet de la classe de modèle qui nous intéresse, ici `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Puis on entraîne cet objet sur les données avec la méthode `fit` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X, y_regress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. On peut accéder au modèle entraîné via des attributs de cet objet. Ici, les paramètres du modèle appris sont accessibles via les attributs `intercept_` (pour $\\beta_0$) et `coef_` (pour $\\boldsymbol{\\beta}$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le poids (g) d'un manchot est prédit par %.2f \" % linreg.intercept_, end='')\n",
    "print(\"+ %.2f x bill_length_mm + %.2f x bill_depth_mm + %.2f x flipper_length_mm\" % (tuple(linreg.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Enfin, on peut utiliser le modèle entraîné pour faire des prédictions en utilisant la méthode `predict` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant s'intéresser à la qualité du modèle. Commençons par visualiser la corrélation entre les valeurs réelles et les valeurs prédites :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_regress, y_pred)\n",
    "\n",
    "plt.xlabel(\"Poids réel (g)\")\n",
    "plt.ylabel(\"Poids prédit (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` nous permet aussi de quantifier la qualité du modèle grâce à différents scores, que l'on trouve dans le module `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour un problème de régression, les métriques disponibles sont listées [ici](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n",
    "\n",
    "Nous allons utiliser ici la racine carrée de l'erreur quadratique moyenne, ou _RMSE_ pour _Root Mean Squared Error_, implémentée dans [`mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La RMSE de notre modèle est %.2f g\" % (metrics.mean_squared_error(y_regress, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! Une erreur d'estimation de 400g n'a pas le même sens selon que l'on parle d'individus pesant plutôt 500g, 5kg ou 50kg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons aussi regarder la corrélation, donnée par le coefficient de détermination $R^2$, implémenté dans [`r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le coefficient de détermination de notre modèle est R2 = %.2f\" % (metrics.r2_score(y_regress, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilisation d'un jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mesures de performance calculées ci-dessus nous permettent de savoir à quel point le modèle « colle » bien aux données.\n",
    "\n",
    "Cependant, elles ne nous informent pas sur la __capacité du modèle à généraliser__, c'est-à-dire à prédire le poids d'un manchot qui ne fasse pas partie de ce jeu de données. C'est cependant ce qui nous intéresse en apprentissage. \n",
    "\n",
    "L'évaluation que nous venons de faire est analogue à poser un examen contenant uniquement des exercices déjà traités en classe : les élèves peuvent réussir cet examen et être tout à fait incapables de résoudre un nouveau problème faisant appel aux mêmes notions.\n",
    "\n",
    "C'est pourquoi on préfère évaluer la performance d'un modèle sur des données qu'il n'a jamais vues auparavant. Pour ce faire, nous allons mettre de côté une partie de nos données, appelées le __jeu de test__, que nous utiliserons uniquement pour l'évaluation. Pensez-y comme à un DS. Nous entraînerons le modèle sur le reste des données, appelé __jeu d'entraînement__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation des données en jeu d'entraînement et jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module [`model_selection`](https://scikit-learn.org/dev/modules/classes.html?highlight=model_selection) de `scikit-learn` propose de nombreuses fonctionalités pour séparer des jeux de données en jeu d'entraînement et jeu de test (ainsi que des modes d'évaluation plus élaborés).\n",
    "\n",
    "Ici nous utilisons `train_test_split` pour séparer les données en un jeu d'entrainement `(X_train, y_train)` contenant 70% des données et un jeu de test `(X_test, y_test)` contenant les 30% restant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X, y_regress, \n",
    "                                                                      test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre `random_state` nous permet de fixer la graine du générateur de nombres aléatoires. La valeur choisie n'a aucun intérêt, mais permet de garantir que si nous réutilisons cette graine à un autre endroit de notre session, nous obtiendrons de nouveau la même séparation en jeu d'entraînement et jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons vérifier le nombre d'observations et de variables dans chacun de ces jeux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le jeu d'entraînement contient %d observations et le jeu de test %d observations.\" % (X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire sur le jeu d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant entraîner une régression linéaire sur le jeu d'entraînement uniquement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance sur le jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous évaluons maintenant la qualité de ce modèle sur le jeu de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test_pred)\n",
    "\n",
    "plt.xlabel(\"Poids réel (g)\")\n",
    "plt.ylabel(\"Poids prédit (g)\")\n",
    "\n",
    "plt.title(\"Prédictions de la régression linéaire sur le jeu de test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La RMSE de notre modèle est %.2f g\" % (metrics.mean_squared_error(y_test, y_test_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le coefficient de détermination de notre modèle est R2 = %.2f\" % (metrics.r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces valeurs sont très proches de celles obtenues sur le jeu d'entraînement ; nous pouvons en conclure qu'il n'y a vraisemblablement pas de sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant travailler sur le Problème 2 défini dans le notebook `0-Donnees.ipynb`. Il s'agit d'un problème de __classification__ sur la même matrice de données $X$ que précédement, mais avec des étiquettes _binaires_, que nous chargeons comme dans le notebook `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classif = pd.Categorical(penguins[\"sex\"]).astype('category').codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation des données en jeu d'entraînement et jeu de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous séparons dès maintenant ce jeu de données en un jeu d'entraînement et un jeu de test.\n",
    "\n",
    "Dans le cas d'un problème de classification, nous allons __stratifier__ les données : cela signifie que nous allons faire en sorte de respecter les proportions relatives d'observations de chacune des classes dans chacun de nos jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X, y_classif, test_size=0.3, random_state=25, \n",
    "                                                                      stratify=y_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalisme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons toujours de $n$ observations en $p$ dimensions, représentés par une matrice $X \\in \\mathbb{R}^{nxp}$ (ici, $p=3$), et d'un vecteur d'étiquettes $\\boldsymbol{y}$. Maintenant $\\boldsymbol{y} \\in \\{0, 1\\}^{n}$. \n",
    "\n",
    "Dans une régression logistique, on modélise la probabilité qu'une observation appartienne à la classe positive, autrement dit ait l'étiquette 1, par une transformation logistique d'une fonction linéaire des variables. Plus précisément, en notant $X$ un vecteur alétaoire réel $p$-dimensionnel qui modélise un manchot et $Y$ une variable aléatoire discrète à valeurs dans $\\{0, 1\\}$ qui modélise son étiquette, on modélise\n",
    "$\\mathbb{P}(Y = 1|X = \\boldsymbol{x})$ par $f(\\boldsymbol{x}),$ avec\n",
    "\n",
    "$$f: \\boldsymbol{x} \\in \\mathbb{R}^p \\mapsto \\sigma\\left(\\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x}\\right),$$\n",
    "\n",
    "où $\\beta_0 \\in \\mathbb{R}$ et $\\boldsymbol\\beta \\in \\mathbb{R}^p$.\n",
    "\n",
    "Pour trouver $\\beta_0$ et $\\boldsymbol\\beta$, on a de nouveau recours à la __minimiation du risque empirique__, calculé avec la __perte logistique__ $(y, f(\\boldsymbol{x})) \\mapsto - y \\log(f(\\boldsymbol{x})- (1-y) \\log(1-f(\\boldsymbol{x}))$ comme fonction de perte. \n",
    "\n",
    "Il s'agit donc de trouver $\\beta_0, \\boldsymbol{\\beta}$ qui minimisent\n",
    "$$ J(\\beta_0, \\boldsymbol{\\beta}) = \\frac1n \\sum_{i=1}^n - y_i \\log(\\sigma\\left(\\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x}_i\\right)) - \n",
    "(1-y_i) \\log(1-\\sigma\\left(\\beta_0 + \\boldsymbol{\\beta}^\\top \\boldsymbol{x}_i\\right)).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique avec scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression logistique est implémentée dans la classe [`LogisticRegression` du module `linear_model`](https://scikit-learn.org/dev/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression) de `scikit-learn`.\n",
    "\n",
    "Nous utilisons une régression logistique « classique », sans pénalisation/régularisation. Il nous faut donc fixer l'argument `penalty` à `none`.\n",
    "\n",
    "Nous suivons exactement les mêmes étapes que pour la régression linéaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instancions un objet de la classe de modèle qui nous intéresse, ici `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(penalty='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entraînons cet objet sur les données d'entraînement avec la méthode `fit` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"La probabilité qu'un manchot soit mâle est prédite par sigma (%.2f \" % logreg.intercept_[0], end='')\n",
    "print(\"+ %.2f x bill_length_mm + %.2f x bill_depth_mm + %.2f x flipper_length_mm)\" % (tuple(logreg.coef_[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Enfin, prédisons les étiquettes des données du jeu de test en utilisant la méthode `predict` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer la performance d'un algorithme de classification, on peut regarder la __matrice de confusion__ des prédictions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(logreg, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d manchots mâles ont été incorrectement prédits femelle.\" % metrics.confusion_matrix(y_test, y_test_pred)[1, 0])\n",
    "print(\"%d manchots femelles ont été incorrectement prédits mâles.\" % metrics.confusion_matrix(y_test, y_test_pred)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance globale du modèle peut aussi être mesurée par son __accuracy__, qui est la proportion de prédictions correctes.\n",
    "\n",
    "Le terme _accuracy_ est généralement traduit par « précision », mais « précision » est aussi la traduction de _precision_, qui est le ratio de prédictions correctes parmi les prédictions positives, c'est-à-dire ici la proportion de manchots mâles parmi les manchots que le modèle a prédit être mâles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.f%% des prédictions du modèle sur le jeu de test sont correctes.\" % (100*metrics.accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Attention !__ Si le nombre d'observations n'est pas le même dans chacune des classes, ce nombre peut être trompeur. Par exemple, si seulement 1% des observations appartiennent à la classe négative, un modèle naïf qui assigne toutes les observations à la classe positive aura une _accuracy_ de 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pour aller plus loin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annexe 1 : Régression linéaire univariée sans scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre but ici va être d'inférer le poids d'un manchot à partir de la longueur de sa palette natatoire. Nous sommes donc dans le cas particulier où $p=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X, y_regress, \n",
    "                                                                      test_size=0.3, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = X_train[:, -1] # on ne garde que la dernière colonne/variable de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_3, y_train)\n",
    "plt.xlabel(\"Longueur de la palette natatoire (mm)\")\n",
    "plt.ylabel(\"Poids (g)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit donc d'apprendre une fonction $f: x \\mapsto a x + b$ telle que $y \\approx f(x_3)$.\n",
    "\n",
    "Ce problème se résout grâce à la méthode des moindres carrés : il s'agit de trouver $a, b$ qui minimisent $J(a, b) = \\frac{1}{n} \\sum_{i=1}^n ((a x_i + b) - y_i)^2$. \n",
    "\n",
    "Il s'agit d'une forme quadratique, convexe, dont on trouve le minimum en annulant sa dérivée en $a$ et en $b$.\n",
    "\n",
    "$\\frac{\\partial J}{\\partial a} (a, b)= \\frac2n \\sum_{i=1}^n x_i^2 a + \\frac2n \\sum_{i=1}^n x_i b - \\frac2n \\sum_{i=1}^n x_i y_i$\n",
    "\n",
    "et\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} (a, b)= b + \\frac2n \\sum_{i=1}^n x_i a - \\frac2n \\sum_{i=1}^n y_i$\n",
    "\n",
    "Ce dont on déduit, à condition que $\\frac1n \\sum_{i=1}^n x_i^2 \\neq \\left(\\frac1n \\sum_{i=1}^n x_i\\right)^2$,\n",
    "\n",
    "$a = \\frac{\\frac1n \\sum_{i=1}^n x_i y_i - \\frac1n \\sum_{i=1}^n x_i \\frac1n \\sum_{i=1}^n y_i}{\\frac1n \\sum_{i=1}^n x_i^2 - \\left(\\frac1n \\sum_{i=1}^n x_i\\right)^2}$ \n",
    "\n",
    "et\n",
    "\n",
    "$b =  \\frac1n \\sum_{i=1}^n y_i - \\frac1n \\sum_{i=1}^n x_i a$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons les termes $\\frac1n \\sum_{i=1}^n x_i$, $\\frac1n \\sum_{i=1}^n x_i^2$, $\\frac1n \\sum_{i=1}^n y_i$, $\\frac1n \\sum_{i=1}^n x_i y_i$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(x_3)\n",
    "x2_mean = np.mean(x_3**2)\n",
    "y_mean = np.mean(y_train)\n",
    "xy_mean = np.mean(x_3 * y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions l'unicité de la solution : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_mean - x_mean**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons les coefficients de notre droite : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (xy_mean - x_mean * y_mean) / (x2_mean - x_mean**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = y_mean - a * x_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant ajouter notre droite de régression au graphique précédent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_3, y_train, label=\"Données\")\n",
    "\n",
    "x_3_min = np.min(x_3) - 5\n",
    "x_3_max = np.max(x_3) + 5\n",
    "plt.plot([x_3_min, x_3_max], [(a * x_3_min + b), (a * x_3_max + b)], lw=2, c='orange', label=\"Modèle\")\n",
    "\n",
    "plt.xlabel(\"Longueur de la palette natatoire (mm)\")\n",
    "plt.ylabel(\"Poids (g)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annexe 2 : Régression linéaire multivariée sans scikit-learn (forme close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant utiliser nos trois variables.\n",
    "\n",
    "Pour simplifier les notations, nous allons ajouter une colonnes de 1 à notre matrice $X$ :\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "    x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "\\end{bmatrix}}_{X} \\to\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "    1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
    "\\end{bmatrix}}_{\\text{$X$ with bias variable}}\n",
    "$$\n",
    "\n",
    "Ce qui nous permet de noter le modèle de la façon suivante :\n",
    "$f: \\boldsymbol{x} \\in \\mathbb{R}^{p+1} \\mapsto \\boldsymbol{\\beta}^\\top \\boldsymbol{x}$ with $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.hstack((np.ones((X_train.shape[0], 1)), X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La minimisation du risque empirique s'écrit alors\n",
    "$$\\boldsymbol{\\beta}^* \\in \\arg\\min \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}_i \\right)^2 = \\frac{1}{n}\\left(\\boldsymbol{y} - X \\boldsymbol\\beta\\right)^\\top(\\boldsymbol{y} - X\\boldsymbol\\beta)$$\n",
    "\n",
    "C'est un problème d'optimisation convexe, que l'on résout en annulant le gradient de la fonction à minimiser.\n",
    "\n",
    "On obtient alors que la solution $\\boldsymbol\\beta^*$ vérifie\n",
    "$(X^\\top X) \\boldsymbol\\beta^* =  X^\\top \\boldsymbol{y}.$\n",
    "\n",
    "__Si $X^\\top X$ est inversible__, on obtient une unique solution\n",
    "$\\boldsymbol\\beta^* = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_star = np.dot(np.linalg.inv(np.dot(X_ones.T, X_ones)), np.dot(X_ones.T, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le poids (g) d'un manchot est prédit par %.2f + %.2f x bill_length_mm + %.2f x bill_depth_mm + %.2f x flipper_length_mm\" % (tuple(beta_star)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annexe 3 : Régression logistique sans scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la régression logistique, la minimisation du risque empirique revient à minimiser\n",
    "\n",
    "$$J(\\boldsymbol\\beta) = \\frac1n \\sum_{i=1}^n - y_i \\log(\\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i))- (1-y_i) \\log(1-\\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i))  $$\n",
    "\n",
    "Comme $\\sigma^\\prime(u) = \\sigma(u) (1-\\sigma(u))$, on obtient comme gradient pour $J$ :\n",
    "$$\\nabla J(\\boldsymbol\\beta) = - \\frac1n \\sum_{i=1}^n \\left(y_i - \\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i) \\right) \\boldsymbol{x}_i$$\n",
    "\n",
    "Cependant, il n'existe pas de solution explcite à \n",
    "$$\\frac1n \\sum_{i=1}^n \\left(y_i - \\sigma(\\boldsymbol\\beta^\\top \\boldsymbol{x}_i) \\right) \\boldsymbol{x}_i = 0,$$\n",
    "we do not have a closed-form solution.\n",
    "\n",
    "Nous avons donc recours à la méthode du gradient.\n",
    "\n",
    "La __méthode du gradient__ permet d'obtenir une solution en progressant itérativement dans la direction opposée au gradient de la fonction à minimiser. Plus précisément, à chaque itération $t$, le vecteur $\\boldsymbol\\beta$ est mis à jour par :\n",
    "$$\\boldsymbol\\beta^{(t+1)} = \\boldsymbol\\beta^{(t)} - \\alpha \\nabla_{\\boldsymbol\\beta} J(\\boldsymbol\\beta^{(t)}).$$\n",
    "où $J: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ est la fonction à minimiser.\n",
    "\n",
    "Dans le cas de la régression linéaire, $d=p+1$ et $J(\\boldsymbol\\beta) = \\frac{1}{n}\\left(\\boldsymbol{y} - X \\boldsymbol\\beta\\right)^\\top(\\boldsymbol{y} - X\\boldsymbol\\beta).$\n",
    "\n",
    "$\\alpha$ est le __pas__ de la descente de gradient ; en apprentissage, on parle de __vitesse d'apprentissage__ (_learning rate_ en anglais).\n",
    "\n",
    "On arrête d'itérer :\n",
    "* soit quand un nombre prescrit d'itérations est atteint ;\n",
    "* soit quand la norme du gradient est moins qu'un seuil prescrit, appelé __tolérance__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est la méthode que nous allons mettre en œuvre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    \"\"\"\n",
    "    Fonction sigmoide.\n",
    "    \n",
    "    Paramètres :\n",
    "    ------------\n",
    "    u: float\n",
    "       Nombre réel.\n",
    "    \"\"\"\n",
    "    return (1./(1+np.exp(-u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(X, y, b_vector):\n",
    "    \"\"\"\n",
    "    Risque empirique pour une régression logistique.\n",
    "    \n",
    "    Paramètres\n",
    "    ----------    \n",
    "    X: (n_samples, n_features+1) numpy array\n",
    "        La matrice de données.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        Le vecteur d'étiquettes\n",
    "        \n",
    "    b_vector: (n_features+1, ) numpy array\n",
    "        Le vecteurs de coefficients de la régression logistique\n",
    "    \"\"\"\n",
    "    # Partie du risque correspondant aux étiquettes positives\n",
    "    where_y_pos = np.where(y==1)[0]\n",
    "    loss_pos = - np.sum(np.log(sigmoid(X[where_y_pos, :].dot(b_vector))))\n",
    "\n",
    "    # Partie du risque correspondant aux étiquettes négatives    \n",
    "    where_y_neg = np.where(y==0)[0]\n",
    "    loss_neg = - np.sum(np.log(1 - sigmoid(X[where_y_neg, :].dot(b_vector))))\n",
    "    \n",
    "    return (loss_pos + loss_neg)/np.size(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gradient_logistic(X, y, b_vector):\n",
    "    \"\"\" \n",
    "    Gradient du risque empirique de la régression logistique\n",
    "    \n",
    "    Paramètres\n",
    "    ----------    \n",
    "    X: (n_samples, n_features+1) numpy array\n",
    "        La matrice de données.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        Le vecteur d'étiquettes\n",
    "        \n",
    "    b_vector: (n_features+1, ) numpy array\n",
    "        Le vecteurs de coefficients de la régression logistique\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    diff = sigmoid(X.dot(b_vector)) - y \n",
    "    return np.sum(np.multiply(diff, X.T), axis=1)/num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logistic(X, y, learning_rate=1e-1, max_iters=30, tol=1e-2):\n",
    "    \"\"\"\n",
    "    Méthode du gradient pour la régression logistique.    \n",
    "    \n",
    "    Paramètres\n",
    "    ----------    \n",
    "    X: (n_samples, n_features+1) numpy array\n",
    "        La matrice de données.\n",
    "        \n",
    "    y: (n_samples, ) numpy array \n",
    "        Le vecteur d'étiquettes\n",
    "        \n",
    "    learning_rate: float\n",
    "        La vitesse d'apprentissage\n",
    "        \n",
    "    max_iters: int\n",
    "        Le nombre maximal d'itérations\n",
    "        \n",
    "    tol: float\n",
    "        La tolérance\n",
    "    \"\"\"\n",
    "    # Initialisation aléatoire des coefficients de régression\n",
    "    beta_current = np.random.rand(X.shape[1])\n",
    "    #beta_current = np.hstack((10*np.random.rand(1), np.random.rand(X.shape[1]-1)))\n",
    "    \n",
    "    # Liste pour stocker les valeurs de la fonction de perte\n",
    "    # à chaque itération\n",
    "    losses = [logistic_loss(X, y, beta_current)]    \n",
    "    # Liste pour stocker les valeurs des coefficients de régression\n",
    "    # à chaque itération\n",
    "    betas = [beta_current.copy()]\n",
    "\n",
    "    for i in range(max_iters):            \n",
    "        # Mise à jour de beta_current\n",
    "        gradient = evaluate_gradient_logistic(X, y, beta_current) \n",
    "        beta_current = beta_current - learning_rate * gradient\n",
    "        \n",
    "        # Ajouter la fonction de perte à la liste losses\n",
    "        losses.append(logistic_loss(X, y, beta_current))\n",
    "        \n",
    "        # Ajouter la valeur des coefficients de régression à la liste betas\n",
    "        betas.append(beta_current.copy())\n",
    "\n",
    "        # Vérifier si la tolérance est atteinte\n",
    "        if np.linalg.norm(gradient) < tol: \n",
    "            break\n",
    "\n",
    "    return(np.array(losses), betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application aux données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X, y_classif, test_size=0.3, random_state=25, \n",
    "                                                                      stratify=y_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.hstack((np.ones((X_train.shape[0], 1)), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 35\n",
    "lr = 2e-4\n",
    "losses, betas = gradient_descent_logistic(X_ones, y_train, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, 'o-')\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"value of the loss\")\n",
    "plt.title(\"Batch gradient descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La probabilité qu'un manchot soit mâle est prédite par sigma(\", end='')\n",
    "print(\"%.2f + %.2f x bill_length_mm + %.2f x bill_depth_mm + %.2f x flipper_length_mm)\" % (tuple(betas[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque :__ Le risque empirique est infini à l'initialisation, d'où un `RuntimeWarning: divide by zero encountered in log`. En pratique, cette approche pourrait être améliorée par :\n",
    "* un pré-traitement des données consistant à centrer-réduire chaque variable ;\n",
    "* l'utilisation d'un taux d'apprentissage variable (élevé au début, plus faible à la fin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ones = np.hstack((np.ones((X_test.shape[0], 1)), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.where(sigmoid(X_test_ones.dot(betas[-1])) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_display = metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_test, y_test_pred))\n",
    "confusion_matrix_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d manchots mâles ont été incorrectement prédits femelle.\" % metrics.confusion_matrix(y_test, y_test_pred)[1, 0])\n",
    "print(\"%d manchots femelles ont été incorrectement prédits mâles.\" % metrics.confusion_matrix(y_test, y_test_pred)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.f%% des prédictions du modèle sur le jeu de test sont correctes.\" % (100*metrics.accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque :__ On confirme ici que la procédure de descente de gradient que nous avons implémentée n'est pas optimale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode du gradient avec données centrées et réduites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons prétraiter les données de manière à ce que chaque variable de notre matrice $\\boldsymbol X$ soit centrée et réduite, c'est à dire que chaque colonne de $\\boldsymbol X$ (à part la première, qui sert de constante) a une moyenne de 0 et un écart-type de 1.\n",
    "\n",
    "Ainsi, l'initialisation de la méthode de gradient fournit des estimées qui sont autour de $0$, c'est-à-dire dans la région où la fonction $\\sigma$ ne sature pas (i.e., son gradient n'est proche de zéro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('means: [%.3f, %.3f, %.3f]' % (tuple(X_scaled.mean(axis = 0))))\n",
    "print('standard deviations: [%.3f, %.3f, %.3f]' % tuple(X_scaled.std(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.hstack((np.ones((X_scaled.shape[0], 1)), X_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 1e-4\n",
    "iterations = 35\n",
    "lr = 2\n",
    "losses, betas = gradient_descent_logistic(X_ones, y_train, learning_rate=lr, max_iters=iterations, tol=tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses, 'o-')\n",
    "\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"value of the loss\")\n",
    "plt.title(\"Batch gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut estimer le modèle suivant : $y \\sim \\sigma(\\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3)$. Or, puisque nous avons effecuté un changement de variable, nous avons à la place estimé le modèle suivant : $$y \\sim \\sigma(\\tilde{\\beta_0} + u_1 \\tilde{\\beta_1} + u_2 \\tilde{\\beta_2} + u_3 \\tilde{\\beta_3}),$$ où $u_i = \\frac{x_i - \\mu_i}{\\sigma_i}$ sont les nouvelles variables, et $\\mu_i$ et $\\sigma_i$ sont les moyennes et écart-types respectifs de $x_i$.\n",
    "\n",
    "Par conséquent, il faut effectuer le changement de variable suivant sur les coefficients obtenus :\n",
    "$$\\beta_0 = \\tilde{\\beta_0} - \\tilde{\\beta_1}\\frac{\\mu_1}{\\sigma_1} - \\tilde{\\beta_2}\\frac{\\mu_2}{\\sigma_2} - \\tilde{\\beta_3}\\frac{\\mu_3}{\\sigma_3},$$\n",
    "$$\\beta_i = \\tilde{\\beta_i} / \\sigma_i, 1\\leq i \\leq 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_scaled = betas[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = beta_scaled[0] - np.sum(beta_scaled[1:4] * scaler.mean_ / scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_original = np.insert(beta_scaled[1:4] / scaler.scale_, 0, beta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La probabilité qu'un manchot soit mâle est prédite par sigma(\", end='')\n",
    "print(\"%.2f + %.2f x bill_length_mm + %.2f x bill_depth_mm + %.2f x flipper_length_mm)\" % (tuple(beta_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque__: Les coefficients sont très éloignés de ceux qu'on avait obtenus sans prétraitement des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance après avoir centré-réduit les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = np.hstack((np.ones((X_test.shape[0], 1)), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.where(sigmoid(X_test_scaled.dot(beta_original)) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_display = metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(y_test, y_test_pred))\n",
    "confusion_matrix_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d manchots mâles ont été incorrectement prédits femelle.\" % metrics.confusion_matrix(y_test, y_test_pred)[1, 0])\n",
    "print(\"%d manchots femelles ont été incorrectement prédits mâles.\" % metrics.confusion_matrix(y_test, y_test_pred)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.f%% des prédictions du modèle sur le jeu de test sont correctes.\" % (100*metrics.accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un bien meilleur modèle avec un preprocessing des données, qui permet, comme nous l'avons vu, d'éviter les problèmes de convergences que peut rencontrer la méthode de descente de gradient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
