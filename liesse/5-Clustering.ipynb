{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage LIESSE: Introduction au clustering avec K-means (K-moyennes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Ce notebook est inspiré des notebooks \n",
    "- https://github.com/Henley13/teaching_td_clustering_2020/blob/main/tp5_clustering_solution.ipynb et \n",
    "- https://jonchar.net/notebooks/k-means/,\n",
    "- ainsi que de l'aide de scikit-learn : https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py et https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means est une méthode de clustering, c'est-à-dire qu'elle a pour but de partitionner les données en différents groupes homogènes, appelés cluster. Contrairement à la classification, le clustering ne dispose pas de labels (mâle/femelle dans les notebooks précédents), mais uniquement des variables.\n",
    "\n",
    "Il s'agit ainsi d'un méthode d'apprentissage non-supervisé : l'on n'a pas de label pour nous guider, et l'on essaye d'obtenir de l'information sur la \"forme\" de notre jeu de donnée (le mot *forme* étant compris dans un sens large et à définir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means partitionne notre jeu de donnée $(\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n)$ en $k$ clusters $(C_1, ..., c_k)$ qui minimisent somme des variance intra-clusters, c'est-à-dire, tels que\n",
    "\n",
    "$$ \\sum_{j = 1}^k \\sum_{\\boldsymbol{x}_i\\in C_j} ||\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j||^2$$\n",
    "\n",
    "soit minimal, sachant que le *centroïde* $\\boldsymbol{\\mu_j}$ est la moyenne (iso-barycentre) des points appartenant au cluster $C_j$.\n",
    "\n",
    "K-means est une méthode itérative et heuristique pour trouver ces clusters. On initialise les centroïdes aléatoirement parmi les points du jeu de données, et on modifie les clusters de la manière itérative suivante :\n",
    "\n",
    "1. Affecter chaque point au cluster dont le centroïde est le plus proche : $$C_j \\gets \\{\\boldsymbol{x}_i | \\forall j' \\neq j, ||\\boldsymbol{x}_i - \\boldsymbol{\\mu}_j||^2 \\leq ||\\boldsymbol{x}_i - \\boldsymbol{\\mu}_{j'}||^2\\}$$\n",
    "\n",
    "2. Recalculer chaque centroïde comme la moyenne des points de son cluster : $$\\mu_j \\gets \\frac{1}{|C_j|}\\sum_{\\boldsymbol{x}_i \\in C_j} \\boldsymbol{x}_i$$\n",
    "\n",
    "L'algorithme s'arrête quand les clusters ne changent plus d'une itération à l'autre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quelques remarques sur K-means:__ \n",
    "1. L'algorithme dépend des centroïdes choisis à l'initialisation et peut rester dans un optimum local : il n'y a pas de garantie que k-means trouve les clusters optimaux. En pratique, on peut lancer l'algorithme K-means avec différents centroïdes initiaux, et choisir un clustering parmi les différents qu'on obtient.\n",
    "2. K-means associe tous les points les plus proches d'un centroide au cluster équivalent. Par conséquent, les clusters sont définis par le diagramme de Voronoi ([définition](https://fr.wikipedia.org/wiki/Diagramme_de_Vorono%C3%AF) et [illustration sur scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py)). Implicitement, K-means fait donc l'hypothèse que les clusters convexes, et ne marchera bien que si cette hypothèse est vérifiée.\n",
    "3. K-means fait aussi l'hypothèse que tous les clusters sont isotropes (c'est-à-dire identiques dans toutes les directions) et ont même variance. [Cet exemple](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py) illustre les problèmes qui interviennent lorsque ces hypothèses ne sont pas vérifiées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means avec scikit-learn (données `penguin`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons illuster K-means sur des données réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"data/penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire la matrice X et le vecteur y\n",
    "X = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]].to_numpy()\n",
    "y = pd.Categorical(penguins[\"species\"]).astype('category').codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous centrons et réduisons chaque variable, de sorte à ce qu'elles aient toutes les deux la même importance et que la longueur du bec ne domine pas sa hauteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cluster.KMeans(n_clusters=3, random_state=34).fit_predict(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut représenter les clusters appris par K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "for label in [0, 1, 2]:\n",
    "    plt.scatter(X[y_pred == label, 0], X[y_pred == label, 1], label=label)\n",
    "plt.title(\"Clustering avec K-means (k = 3)\")\n",
    "plt.xlabel(\"Longueur du bec (mm)\")\n",
    "plt.ylabel(\"Hauteur du bec (mm)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "for i, label in enumerate(list(set(penguins.loc[:, \"species\"]))):\n",
    "    data_label = X[penguins.loc[:, \"species\"] == label, :]\n",
    "    plt.scatter(data_label[:, 0], data_label[:, 1],\n",
    "                s=30, label=label)\n",
    "plt.title(\"Espèces de manchot\")\n",
    "plt.xlabel(\"Longueur du bec (mm)\")\n",
    "plt.ylabel(\"Hauteur du bec (mm)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les clusters semblent correspondre assez bien aux espèces de pingouins ! Dans la section suivante nous allons quantifier cette correspondance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesurer la similarité entre deux clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voulons mesurer à quel point le clustering obtenu par k-means est proche des vrais classes. Pour ce faire, nous utilisons le l'index de Rand. \n",
    "Considérons deux partitionnements des données $\\mathcal{P}_1$ $\\mathcal{P}_2$, qui correspondent à deus clustering. Définissons le quantités\n",
    "- $a$: nombre de paires de points qui sont dans le même cluster pour $\\mathcal{P}_1$ et pour $\\mathcal{P}_2$.\n",
    "- $b$: nombre de paires de points qui sont dans des clusters différents pour $\\mathcal{P}_1$ et pour $\\mathcal{P}_2$.\n",
    "- $c$: nombre de paires de points qui sont dans le même cluster pour $\\mathcal{P}_1$ et pour des clusters différents pour $\\mathcal{P}_2$.\n",
    "- $d$: nombre de paires de points qui sont dans des clusters différents pour $\\mathcal{P}_1$ et dans le même cluster pour $\\mathcal{P}_2$.\n",
    "On voit que $a + b$ est le nombre de paires concordantes entre les deux partitions et $c + d$ représente le nombre de paires discordantes entre les deux partitions.\n",
    "\n",
    "Nous définissons alors\n",
    "$$R = \\frac{a+b}{a+b+c+d} = \\frac{a+b}{n\\choose{2}}.$$\n",
    "\n",
    "L'index de Rand est compris entre $0$ et $1$ et permet de mesurer la concordance entre deux clusterings. Nous pouvons aussi l'utiliser pour mesurer la concordance entre les espèces de pingouins et les clusters formés par le K-means.\n",
    "\n",
    "Attention, il ne s'agit pas d'apprentissage supervisé ! Ces étiquettes (espèces de pingouins) n'ont pas été utilisés pour entraîner la méthode de clustering. Et nous ne pouvons pas avoir recours à une mesure de performance de classification, car les clusters peuvent concorder parfaitement avec les classes mais avoir des numéros différents (le cluster 1 concorde avec la classe 2, etc.) ce qui donnerait lieu à une matrice de confusion avec des 0 sur la diagonales.\n",
    "\n",
    "Nous pouvons utiliser l'indice de Rand parce que nous savons qu'il y a trois vraies classes dans nos données. Dans les applications pratiques, le clustering n'est pas utilisé pour trouver des classes qui correspondent à des labels dont l'on disposerait (sinon, on utiliserait de la classification supervisée)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un score élevé. L'index de Rand est plus faible pour le clustering des données penguins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L'index de Rand des clusters de manchots est %.2f.\" % metrics.rand_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser l'index de Rand ajusté ([https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html](réference)), qui compense le fait que deux clusterings choisis aléatoirements n'ont pas un index de Rand nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"L'index de Rand ajusté des clusters de manchots est %.2f.\" % metrics.adjusted_rand_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons des clusters assez proches des vrais clusters. Peut-on mesurer la qualité de ce clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir le nombre de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour choisir le nombre de clusters, nous allons utiliser le coefficient de silhouette ([référence](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)). Le coefficient de silhouette est un critère géométrique qui quantifie à quel point chaque observation est bien intégrée dans son cluster.\n",
    "\n",
    "Soit $a(\\boldsymbol{x})$ la distance moyenne intra-cluster pour le point $\\boldsymbol{x}$:\n",
    "$$a(\\boldsymbol{x}) = \\frac{1}{|C_k| - 1} \\sum_{\\boldsymbol{x'}\\in C_k \\backslash \\boldsymbol{x}} ||\\boldsymbol{x} - \\boldsymbol{x'}||.$$ \n",
    "Soit $b(\\boldsymbol{x})$ la distance moyenne au plus proche cluster:\n",
    "$$b(\\boldsymbol{x}) = \\min_{l\\neq k} \\frac{1}{|C_k| - 1} \\sum_{\\boldsymbol{x'} \\in C_l} ||\\boldsymbol{x} - \\boldsymbol{x'}||.$$ \n",
    "$a(\\boldsymbol{x})$ quantifie à quel point $\\boldsymbol{x}$ est bien intégré à son cluster et $b(\\boldsymbol{x})$ quantifie à quel point $\\boldsymbol{x}$ serait bien intégré à un autre cluster.\n",
    "\n",
    "Le coefficient de silhouette est défini par:\n",
    "$$s(\\boldsymbol{x}) = \\frac{b(\\boldsymbol{x}) - a(\\boldsymbol{x})}{\\max(a(\\boldsymbol{x}), b(\\boldsymbol{x})}.$$\n",
    "Il est compris entre $-1$ (si $\\boldsymbol{x}$ est très proche des points d'un autre cluster) et $1$ (si $\\boldsymbol{x}$ est très proche des points de son cluster).\n",
    "\n",
    "La somme $s = \\sum_{\\boldsymbol{x}} s(\\boldsymbol{x})$ des scores de silhouette donne un score global de la « qualité » du clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant utiliser ce score pour comparer les clusterings obtenus avec K-means pour différents nombres de clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clusters = [2, 3, 4, 5]\n",
    "y_pred = [] # liste de résultats de clusterings\n",
    "for k in nb_clusters:\n",
    "    y_pred_k = cluster.KMeans(n_clusters=k, random_state=34).fit_predict(X_std)\n",
    "    y_pred.append(y_pred_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in np.arange(len(nb_clusters)):\n",
    "    axes[idx].scatter(X[:, 0], X[:, 1], \n",
    "                    c=y_pred[idx])\n",
    "    axes[idx].set_title(\"k = %d, silhouette = %.2f\" % \\\n",
    "          (nb_clusters[idx], metrics.silhouette_score(X_std, y_pred[idx])))\n",
    "    axes[idx].set_xlabel(\"Longueur du bec (mm)\")\n",
    "    axes[idx].set_ylabel(\"Hauteur du bec (mm)\")\n",
    "    \n",
    "fig.tight_layout() # adjust spacing between subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longueur et hauteur de bec séparent bien mieux les données en 3 clusters (silhouette = 0.52) qu'en 2, 4 ou 5 clusters (silhouete < 0.52) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de K-means en python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques fonctions utiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser les clusters aléatoirement\n",
    "def initialize_clusters(points, k, random_state = None):\n",
    "    \"\"\"Initializes clusters as k randomly selected points from points.\"\"\"\n",
    "    if random_state is not None:\n",
    "        numpy.random.seed(random_state)\n",
    "    return points[np.random.randint(points.shape[0], size=k)]\n",
    "    \n",
    "# Calcul des distances entre points de centroides\n",
    "def get_distances(centroid, points):\n",
    "    \"\"\"Returns the distance the centroid is from each data point in points.\"\"\"\n",
    "    return np.linalg.norm(points - centroid, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "max_iter = 50\n",
    "\n",
    "# Initialize our centroids by picking random data points\n",
    "centroids = initialize_clusters(X, k, random_state=17)\n",
    "\n",
    "# Initialize the vectors in which we will store the\n",
    "# assigned classes of each data point and the\n",
    "# calculated distances from each centroid\n",
    "classes = np.zeros(X.shape[0], dtype=np.float64)\n",
    "distances = np.zeros([X.shape[0], k], dtype=np.float64)\n",
    "\n",
    "## Define oject to keep the kmeans iterations results in memory\n",
    "classes_all = np.zeros((X.shape[0], max_iter), dtype = np.int16)\n",
    "centroids_all = np.zeros((centroids.shape[0], centroids.shape[1], max_iter))\n",
    "\n",
    "# Loop for the maximum number of iterations\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # Assign all points to the nearest centroid\n",
    "    for j, c in enumerate(centroids):\n",
    "        distances[:, j] = get_distances(c, X)\n",
    "        \n",
    "    # Determine class membership of each point\n",
    "    # by picking the closest centroid\n",
    "    classes = np.argmin(distances, axis=1)\n",
    "    classes_all[:, i] = classes\n",
    "    \n",
    "    # Update centroid location using the newly\n",
    "    # assigned data point classes\n",
    "    for c in range(k):\n",
    "        centroids[c] = np.mean(X[classes == c], 0)\n",
    "    centroids_all[:, :, i] = centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons afficher le résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [0, 1, 2]:\n",
    "    plt.scatter(X[classes == label,0], X[classes == label, 1], alpha=0.5)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], color=['blue', 'darkred', 'green'], marker='s', lw=2)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title(\"Clustering obtained by scikit-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons aussi représenter les différentes étapes de K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "for ind in range(0, 6):\n",
    "    plt.subplot(3, 2, ind + 1)\n",
    "    for label in [0, 1, 2]:\n",
    "        plt.scatter(X[classes_all[:, ind] == label,0], X[classes_all[:, ind] == label, 1], alpha=0.5)\n",
    "    \n",
    "    plt.scatter(centroids_all[:,0 , ind], centroids_all[:, 1, ind],\\\n",
    "                color=['blue', 'darkred', 'green'], marker='s', lw=2)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Iteration #\" + str(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données simulées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module `datasets` de scikit-learn permet aussi de __simuler__ des données, ce qui permet de créer des exemples didactiques. Nous l'illustrons ici sur l'exemple du clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous générons des données à partir d'un mélange de trois distributions gaussiennes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "X, label = datasets.make_blobs(n_samples=n_samples, \n",
    "                                  cluster_std=[1.0, 2.5, 0.5], \n",
    "                                  random_state=170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On représente ces données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blobs = pd.DataFrame({\"x1\": X[:, 0],\n",
    "                         \"x2\": X[:, 1],\n",
    "                         \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for i in [0, 1, 2]:\n",
    "    plt.scatter(X[label == i, 0], X[label == i, 1], label = i)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Labels utilisés pour générer les données\")\n",
    "plt.legend(prop={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = cluster.KMeans(n_clusters = 3, random_state = 45).fit_predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "for i in [0, 1, 2]:\n",
    "    plt.scatter(X[clusters == i, 0], X[clusters == i, 1], label = i)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Cluster obtenus par K-Means\")\n",
    "plt.legend(title = \"clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sur les données simulées, utiliser le score de silhouette pour vérifier qu'il est préférable de choisir 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observez l'importance d'avoir centré-réduit les variables en répétant l'analyse des manchots _sans centrer-réduire les variables_, c'est-à-dire en utilisant `X` et non pas `X_std`.\n",
    "\n",
    "3. D'autres méthodes de clustering sont présentées ici : [https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods](ici).\n",
    "\n",
    "- En observant les résultats des différentes méthodes sur le jeu de données à la première rangée, quelles méthodes semblent ne pas faire l'hypothèse que les clusters sont convexes ?\n",
    "- En observant les résultats des différentes méthodes sur la quatrième rangée, quelles méthodes semblent ne pas faire l'hypothèse que les clusters sont isotropes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Distribution des silhouettes\n",
    "\n",
    "Pour aller plus loin, on peut vouloir représenter la *distribution* des silhouettes de chaque cluster, au lieu d'uniquement calculer leurs moyennes. Nous utilisons pour cela la fonction `silhouette_samples` (ici sur les données simulées) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input values\n",
    "X = df_blobs.loc[:, [\"x1\", \"x2\"]].values\n",
    "\n",
    "# one silhouette plot for different numbers of clusters.\n",
    "for n_clusters in [2, 3, 4, 9]:\n",
    "    \n",
    "    # create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 4)\n",
    "    \n",
    "    # demarcate silhouette plots of individual clusters by inserting blanck space\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # K-means clustering\n",
    "    kmean = cluster.KMeans(\n",
    "        n_clusters=n_clusters, init='k-means++', \n",
    "        n_init=10, max_iter=300, tol=0.0001, \n",
    "        verbose=0, random_state=13, \n",
    "        algorithm='auto')\n",
    "    cluster_labels = kmean.fit_predict(X)\n",
    "\n",
    "    # compute average silhouette score\n",
    "    score = metrics.silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # compute silhouette scores for each sample\n",
    "    sample_silhouette_values = metrics.silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    # plot silhouette scores for each sample\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        # aggregate the silhouette scores for samples belonging to cluster i and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    # axis labels and title\n",
    "    ax1.set_title(\"Average silhouette coefficient: {0:0.3f}\".format(score), fontsize=10)\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize=15)\n",
    "\n",
    "    # draw a vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=score, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # clear the yaxis labels / ticks\n",
    "    ax1.set_yticks([])  \n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], \n",
    "                marker='o', s=30, alpha=0.8, c=colors, edgecolor='k')    \n",
    "    \n",
    "    # draw white circles at cluster centers\n",
    "    centers = kmean.cluster_centers_\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "        \n",
    "    # axis labels and title\n",
    "    ax2.set_title(\"Number of clusters: {0}\".format(n_clusters), fontsize=15)\n",
    "    ax2.set_xlabel(\"X1\", fontsize=10)\n",
    "    ax2.set_ylabel(\"X2\", fontsize=10)\n",
    "\n",
    "    # main title\n",
    "    plt.suptitle((\"Silhouette analysis for K-means clustering on sample data \"\n",
    "                  \"with n_clusters = {0}\".format(n_clusters)),\n",
    "                 fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que la distribution des score de silhouette dans chaque cluster représente bien à quel point le cluster est \"clairement identifié\" comparé aux autre clusters. \n",
    "\n",
    "Lorsque beaucoup de points dans un cluster a un coefficient de silhouette élevé, la distribution de coefficients est concentrée autour d'une valeur élevée, comme c'est le cas avec $k = 2$ et $k = 3$ clusters (correspondant aux figures des deux lignes du haut).\n",
    "\n",
    "En revanche, lorsqu'un clusters a beaucoup de coefficients de silhouette autour de valeurs faible, le discribution des coefficients est presque uniforme entre $0$ et une valeur faiblement élevée ($\\sim 0.5$), comme c'est le cas avec $k=4$ et $k = 9$ clusters (correspondant aux figures des deux lignes du bas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
