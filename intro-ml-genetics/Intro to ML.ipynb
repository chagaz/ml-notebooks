{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning \n",
    "\n",
    "Notebook prepared by [Chloé-Agathe Azencott](http://cazencott.info), with thanks to [Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook). Data from the GEMLeR (Gene Expression Machine Learning Repository) maintained by [Gregor Stiglic](http://www.ri.fzv.um.si/gstiglic/).\n",
    "\n",
    "In this notebook, we'll try to build a classifier that automatically separates breast cancer tumor from ovarian cancer tumors, from the gene expression (microarray data) of 3,000 genes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preamble\n",
    "## 1.1 What is Jupyter? \n",
    "\n",
    "A Jupyter notebook is a web application that allows you to create and share documents (such as this `.ipynb` notebook) that contain live code, visualisations and explanatory text (with equations).\n",
    "\n",
    "Here are some tips on using a Jupyter notebook:\n",
    "* Each block of text is contained in a _cell_. A cell can be either raw text, code, or markdown text (such as this cell). For more info on markdown syntax, follow the [guide](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "* You can run a cell by clicking inside it and hitting `Shift+Enter` (or the play button in the toolbar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "2 + 2  # hit Shift+Enter to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to create a new cell below the one you're running, hit `Alt+Enter` (or the plus button in the toolbar).\n",
    "\n",
    "Some tips on using a Jupyter notebook with Python:\n",
    "* A notebook behaves like an interactive python shell! This means that\n",
    "    * classes, functions, and variables defined at the cell level have global scope throughout the notebok\n",
    "    * hitting `Tab` will autocomplete the keyword you have started typing\n",
    "    * typing a question mark after a function name will load the interactive help for this function.\n",
    "* Jupyter has special Python commands (shortcuts, if you will) called _magics_. For instance, `%bash` will allow you to run bash code, `%paste` will allow you to paste a block of code while retaining its formating, and `%matplotlib inline` will import the visualization library matplotlib, and automatically display its plots inline, that is, below the cell. Here's a full list: http://ipython.readthedocs.io/en/stable/interactive/magics.html \n",
    "* Learn more about the interactive Python shell here: http://ipython.readthedocs.io/en/stable/interactive/tutorial.html\n",
    "\n",
    "For more info on Jupyter: https://jupyter.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this notebook on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "\n",
    "Here are several things you will need that are specific to Google Colab:\n",
    "* Make sure to download the data file `small_Breast_Ovary.csv` locally (to your computer), and upload it again on Colab.\n",
    "* You may need to force Colab to use Python 3.7. To do so, uncomment (that is to say, remove the `# `) and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install python3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be able to run this notebook on your own machine, here's what you'll need:\n",
    "\n",
    "__Option 1:__ If you are familiar with python and comfortable with managing your own installation, make sure you have Python 3.7 installed and the following packages (all can be installed with pip): numpy, scipy, pandas, matplotlib, scikit-learn, jupyter and jupyterlab.\n",
    "\n",
    "__Option 2:__ If you are not familiar with python and library management, we recommand using either\n",
    "* miniconda: https://docs.conda.io/en/latest/miniconda.html\n",
    "* anaconda: https://www.anaconda.com/distribution/\n",
    "Miniconda is lighter, but you will need to make sure all the required packages are installed; anaconda is heavier (requires a few GB of space) but everything should work “out of the box”.\n",
    "\n",
    "Make sure to follow the installation instructions for your operating system (Mac/Windows/Linux) and install the Python 3.7 version.\n",
    "\n",
    "If you’re unsure whether your Windows machine is running a 32-bit or 64-bit system, you can use the instructions here: https://www.lifewire.com/am-i-running-a-32-bit-or-64-bit-version-of-windows-2624475 to check. If you have a 32-bit version, you’ll need to use miniconda. For Linux, run “uname -i” in a terminal. If the answer is x86_64, you have a 64-bit system; if it is i386 or i686, you have a 32-bit system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data science libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us start with the Jupyter magic \"`%pylab inline`\", which is equivalent to importing `numpy` as `np`, and importing `matplotlib` as `plt`. \n",
    "\n",
    "`numpy` (for \"numeric python\") is the library used for manipulating arrays (typically representing vectors and matrices) in Python. To access object `a_numpy_object` from `numpy`, we'll use `np.a_numpy_object`.\n",
    "\n",
    "`matplotlib` is a plotting library inspired by Matlab.\n",
    "\n",
    "The `inline` specifier makes it so that the plots will appear under the cell and not in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This command is equivalent to:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.plot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will also import the `pandas` library, which is very useful for data manipulation.\n",
    "\n",
    "__Documentation:__ http://pandas.pydata.org/pandas-docs/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all our machine learning purposes, we will use the libray `scikit-learn`: https://scikit-learn.org/stable/index.html\n",
    "Its documentation is very complete! Don't hesitate to refer to it extensively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, each observation is a tumor, and it is described by the expression of 3,000 genes. There are two types of tumors: breast tumors and ovary tumors. Our goal will be to build a tumor classifier based on gene expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvo_df = pd.read_csv('small_Breast_Ovary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column (\"ID_REF\") contains the sample ID, the last one (\"Tissue\") the \"Breast\" or \"Ovary\" label, and all others are gene expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data in numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information describing the samples can be thought of as a two-dimensional numerical array or matrix, which we will call the __design matrix.__ By convention, this  matrix is often stored in a variable named `X`. It is assumed to be two-dimensional, with shape `(n_samples, n_features)`, contained in a NumPy array.\n",
    "\n",
    "The samples (i.e., rows) always refer to the individual objects described by the dataset; here, our tumors. \n",
    "\n",
    "The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner; here, the transcript levels.\n",
    "\n",
    "In addition to the feature matrix X, we also work (in supervised learning) with a NumPy array containing the labels (or targets), which we will usually call `y`. It is stored as a one-dimensional NumPy array of shape `(n_samples, )`. This __target array__ may have continuous numerical values, or discrete classes/labels. This array contains the variable we want to _predict_, by opposition to the features matrix, which contain the variables we want to _use to make our predictions_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract these arrays from the `bvo_df` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design matrix\n",
    "X = np.array(bvo_df.drop(columns=[\"ID_REF\", \"Tissue\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 542 samples, each represented by 3000 gene expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target array\n",
    "y = np.array(bvo_df[\"Tissue\"])\n",
    "\n",
    "# convert \"Breast\" in 0 and the other labels (here, \"Ovarian\") into 1\n",
    "y = np.where(y=='Breast', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standardization\n",
    "\n",
    "Let us make sure our features all have a mean of 0 and a standard deviation of 1: this will avoid giving too much importance to genes that are more abundant across the whole data set.\n",
    "\n",
    "This can easily be done with scikit-learn's [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate a scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then compute the scaling parameters on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a scaled version of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a logistic regression\n",
    "In this section you will learn how to train a logistic regression on this data.\n",
    "\n",
    "All machine learning algorithms implemented in scikit-learn follow the same logic:\n",
    "\n",
    "1. Choose an algorithm and import the appropriate class from scikit-learn\n",
    "\n",
    "The [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#), which is a linear model, is part of the `linear_model` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instantiate this class with desired hyperparameters.\n",
    "\n",
    "Here, we don't want to use regularization yet, so we'll use `penalty='None'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = linear_model.LogisticRegression(penalty='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit the model to the data using `fit()`.\n",
    "\n",
    "At this stage, our model has not seen any data. Now we'll pass it the data we want it to learn on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned a model! In the case of linear models, we can inspect its coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), my_model.coef_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the logistic model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of these coefficients are very close to zero... this is why we'll use regularization later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use the model to make predictions with `predict()`.\n",
    "\n",
    "Here we'll make predictions on the data we used to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = my_model.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a lot of ways to evaluate predictions in its [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics) module.\n",
    "\n",
    "We can for example look at the accuracy of our model: what proportion of the samples did it predict correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the logistic regression: %.3f\" % metrics.accuracy_score(y, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wow!__ Our model made perfect predictions!\n",
    "\n",
    "True, but that's easy to do on the data it learned from... Think of it as being tested on the exact same exercises you did in class — not the same as being able to solve a brand new problem, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using a test set\n",
    "\n",
    "It's much more realistic to evaluate the performance of a model on data it has never seen before. For that reason, we're going to set aside a chunk of our data, called the __test set__, which we'll only use for evaluation purposes. We'll train on our model on the rest of the data, called the __train set__.\n",
    "\n",
    "## 3.1 Splitting the data into a train and a test set\n",
    "\n",
    "Scikit-learn provide utilities to create train and test sets (and more complex evaluation/validation set ups) in the `model_selection` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = model_selection.train_test_split(X_scaled, y, \n",
    "                                                                      test_size=0.2, \n",
    "                                                                      stratify=y # stratifying means respecting the proportion of samples of each class in all sets\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_size=0.2` means the test set will be 20% of the full set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stratify=y` means the relative proportions of samples of each class in `y` will be respected in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set contains 433 samples ; the test set contains 109 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training on the train set only\n",
    "\n",
    "We can now train on logistic regression on the train set only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluation on the test set\n",
    "\n",
    "Let us now use this model to make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the logistic regression: %.3f\" % metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but it's not perfect any longer.\n",
    "\n",
    "To understand this performance in more depth, we can look at the __confusion matrix__ of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(my_model, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottom left cell contains the number of tumors that were predicted to be from breast cancer (Predicted label=0), whereas they were ovarian cancer (True label=1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look a the model coefficients again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), my_model.coef_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the logistic model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these coefficients are very close to 0. Using a logistic regression with l1 regularization can bring down these coefficients to exactly zero, resulting in a _sparse_ model. Then we can make the hypothesis that only the genes that have non-zero coefficients in the model are relevant to the prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training a regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l1_regularized_model = linear_model.LogisticRegression(penalty='l1',\n",
    "                                                         solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`solver='liblinear'` tells scikit-learn which optimization algorithm to use to fit the model. The default solver is not compatible with l1 regularization, so here we need to explicitely set a solver that can be used for l1 regularization. You can learn more about it [in the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l1_regularized_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Effect of the regularization on the model coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at the model's coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), my_l1_regularized_model.coef_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(my_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the difference more clear, we'll plot in two different colors the coefficients that are equal to zero and those that aren't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# First subplot in a (1 x 2) grid\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "nonzero_coefficents_indices = np.nonzero(my_model.coef_)\n",
    "plt.scatter(nonzero_coefficents_indices[1], \n",
    "            my_model.coef_[nonzero_coefficents_indices], label='Non-zero coefficients')\n",
    "zero_coefficients_indices = np.nonzero(my_model.coef_ == 0)\n",
    "plt.scatter(zero_coefficients_indices[1], \n",
    "           my_model.coef_[zero_coefficients_indices], label='Zero coefficients')\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"Logistic regression\")\n",
    "\n",
    "# Second subplot in a (1 x 3) grid\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "nonzero_coefficents_indices = np.nonzero(my_l1_regularized_model.coef_)\n",
    "plt.scatter(nonzero_coefficents_indices[1], \n",
    "            my_l1_regularized_model.coef_[nonzero_coefficents_indices], label='Non-zero coefficients')\n",
    "zero_coefficients_indices = np.nonzero(my_l1_regularized_model.coef_ == 0)\n",
    "plt.scatter(zero_coefficients_indices[1], \n",
    "            my_l1_regularized_model.coef_[zero_coefficients_indices], label='Zero coefficients')\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without logistic regression, there is no feature that has exactly a coefficient of zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_l1log = my_l1_regularized_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the l1-regularized logistic regression: %.3f\" % metrics.accuracy_score(y_test, y_predicted_l1log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(my_l1_regularized_model, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Effect of the amount of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large regularization\n",
    "\n",
    "We have used the default setting for the inverse of the regularization strength parameter `C`. \n",
    "\n",
    "However, changing this hyperparameter has a strong effect: the stronger the regularization (i.e. the smaller `C`), the more coefficients will be set to zero in the model. \n",
    "\n",
    "We can observe this by reiterating the above experiment with `C=0.01`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l1_regularized_model_2 = linear_model.LogisticRegression(penalty='l1',\n",
    "                                                            solver='liblinear', \n",
    "                                                           C=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l1_regularized_model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the coefficients now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 3))\n",
    "\n",
    "# First subplot in a (1 x 3) grid\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "nonzero_coefficents_indices = np.nonzero(my_model.coef_)\n",
    "plt.scatter(nonzero_coefficents_indices[1], \n",
    "            my_model.coef_[nonzero_coefficents_indices], label='Non-zero coefficients')\n",
    "zero_coefficients_indices = np.nonzero(my_model.coef_ == 0)\n",
    "plt.scatter(zero_coefficients_indices[1], \n",
    "           my_model.coef_[zero_coefficients_indices], label='Zero coefficients')\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"Logistic regression\")\n",
    "\n",
    "# Second subplot in a (1 x 3) grid\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "nonzero_coefficents_indices = np.nonzero(my_l1_regularized_model.coef_)\n",
    "plt.scatter(nonzero_coefficents_indices[1], \n",
    "            my_l1_regularized_model.coef_[nonzero_coefficents_indices], label='Non-zero coefficients')\n",
    "zero_coefficients_indices = np.nonzero(my_l1_regularized_model.coef_ == 0)\n",
    "plt.scatter(zero_coefficients_indices[1], \n",
    "            my_l1_regularized_model.coef_[zero_coefficients_indices], label='Zero coefficients')\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression (C=1.0)\")\n",
    "\n",
    "# Third subplot in a (1 x 3) grid\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "nonzero_coefficents_indices = np.nonzero(my_l1_regularized_model_2.coef_)\n",
    "plt.scatter(nonzero_coefficents_indices[1], \n",
    "            my_l1_regularized_model_2.coef_[nonzero_coefficents_indices], label='Non-zero coefficients')\n",
    "zero_coefficients_indices = np.nonzero(my_l1_regularized_model_2.coef_ == 0)\n",
    "plt.scatter(zero_coefficients_indices[1], \n",
    "            my_l1_regularized_model_2.coef_[zero_coefficients_indices], label='Zero coefficients')\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression (C=0.01)\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more coefficients are equal to zero now. How did this affect the prediction performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_l1log_2 = my_l1_regularized_model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the l1-regularized logistic regression (C=0.01): %.3f\" % metrics.accuracy_score(y_test, y_predicted_l1log_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(my_l1_regularized_model_2, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the amount of regularization reduces the number of features used by the model, but this can also hurt performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Setting the amount of regularization by cross-validation\n",
    "\n",
    "We now want to perform __model selection__, that is to say, _select_ the best value of `C`. \n",
    "\n",
    "One way to approach the problem would be to test several values for `C` and compare performance on the test set. We would then pick the value of `C` leading to the best performance. Unfortunately, if we proceed in this way, the performance we observe on the test set is biased: it's not true any more to say that we have not touched the test set to create our model! \n",
    "\n",
    "What we could do now is split the training set into two sets again: a train set and a validation set. \n",
    "\n",
    "However, if we split the data in, say, 60% train + 20% validation + 20% test, now we're only using little more than half our data for training! This is not optimal, especially if the initial set of training data is small, because the more data we have, the better we learn. \n",
    "\n",
    "One way to address this is to use __cross-validation__; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set. If we do a 5-fold cross-validation, we split the data in 5 blocks, and run 5 experiments for each value of `C` that we want to test: use the first 4 blocks for training and the last one for validation ; use the three first blocks and the last one for training, and the fourth one for validation ; and so on and so forth. We end up with 5 measures of performance for each value of `C`, which we can then average to get a global picture of the performance (still for each value of `C`). We can now pick the value of `C` that led to the best performance, train our model again on the training set, and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated model selection by cross-validation with `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by setting up a grid of values of `C`. We'll use 50 values, spread on a logarithmic scale between 1e-3 and 1e3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-3, 3, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use scikit-learn's `GridSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv = model_selection.GridSearchCV(linear_model.LogisticRegression(penalty='l1', solver='liblinear'), \n",
    "                                                 {'C': C_values},\n",
    "                                                 cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{'C': C_values}` tells scikit-learn that it will have to consider all models `linear_model.LogisticRegression(penalty='l1', solver='liblinear', C=xxx)` with `xxx` in `C_values`. \n",
    "\n",
    "`cv=5` tells scikit-learn to use a 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of the hyperparameter is in the `best_params_` attribute of our trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding performance in the `best_score_` attribute of our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, but what measure of performance is this? The [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) tells us that this can be set with the `scoring` parameter of `GridSearchCV`, which we did not touch. So the default was used — the documentation reads \"If None, the estimator’s score method is used.\" So let's look up the documentation of [LogisticRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#); it tells us that its `score` method returns the mean accuracy, so that's what we're looking at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn has also retrained a l1-regularized logistic regression with the optimal hyperparameter. It is accessible in the `best_estimator_` attribute of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the weights of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), l1_regularized_cv.best_estimator_.coef_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the performance of this model on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_l1log_cv = l1_regularized_cv.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the l1-regularized logistic regression (optimal C): %.3f\" % metrics.accuracy_score(y_test, y_predicted_l1log_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(l1_regularized_cv.best_estimator_, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of selected features is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(l1_regularized_cv.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scoring for an unbalanced data set\n",
    "\n",
    "We used accuracy to select the best model. However, the data is _unbalanced_: there are more breast tumors than ovarian tumors. Let us check their numbers in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of breast tumors = %d (%.2f %%  of the training set)\" % ((np.count_nonzero(y_train==0)), 100*(np.count_nonzero(y_train==0)/y_train.shape[0])))\n",
    "print(\"Number of ovarian tumors = %d (%.2f %% of the training set)\" % (np.count_nonzero(y_train==1), 100*(np.count_nonzero(y_train==1)/y_train.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that, on the training set, a model that sytematically returns 0 (i.e. \"breast\") will have an accuracy of 63.5%.\n",
    "\n",
    "The imbalance in the data means that models will tend to favor the majority class.\n",
    "\n",
    "To avoid this, we can use a performance score that accounts for this imbalance. These include the __balanced accuracy__ and the __f1__ score. You can learn more about them in the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Balanced accuracy of the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy of the l1-regularized logistic regression (optimal C): %.3f\" % metrics.balanced_accuracy_score(y_test, y_predicted_l1log_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Optimizing for balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv_ba = model_selection.GridSearchCV(linear_model.LogisticRegression(penalty='l1', solver='liblinear'), \n",
    "                                                   {'C': C_values},\n",
    "                                                   cv=5, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv_ba.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv_ba.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv_ba.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularized_cv_ba.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the weights of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), l1_regularized_cv_ba.best_estimator_.coef_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Coefficient in the model\")\n",
    "plt.title(\"L1-regularized logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(l1_regularized_cv_ba.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the performance of this model on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_l1log_cv = l1_regularized_cv_ba.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy of the l1-regularized logistic regression (optimal C): %.3f\" % metrics.balanced_accuracy_score(y_test, y_predicted_l1log_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(l1_regularized_cv_ba.best_estimator_, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Decision trees and random forest classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 6.1 Decision tree\n",
    "\n",
    "Let us start with a simple non-linear models: a decision tree. They are implemented in scikit-learn's [tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a DT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dt_model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a decision tree on the train set only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use this model to make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the decision tree is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the decision tree: %.3f\" % metrics.accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy of the decision tree: %.3f\" % metrics.balanced_accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(dt_model, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree clearly underperforms compared to a logistic regression.\n",
    "\n",
    "Can we improve this with __ensemble methods__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest combines the prediction of multiple decision trees, each trained on a subset of the samples and of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Can an ensemble method improve the performance of the decision tree on the difficult data set? We will use the random forest implementation in scikit-learn's [ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important hyperparameter of a random forest is the number `n_estimators` of trees it contain. We will therefore use a cross-validation to fix this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees_values = [10, 20, 50, 100, 300, 500, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = model_selection.GridSearchCV(ensemble.RandomForestClassifier(),  \n",
    "                                     {'n_estimators': ntrees_values},\n",
    "                                     cv=5, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of trees is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal cross-validated balanced accuracy: %.3f\" % rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = rf_cv.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy of the random forest: %.3f\" % metrics.balanced_accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(rf_cv.best_estimator_, X_test, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is much better than that of a single decision tree, and is also better than that of the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Random forests have a notion of _feature importance_, stored in the `feature_importances_` attribute. The importance of a feature is computed by looking at how much using that feature decreases the Gini impurity (a measure of classification error) of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(n_features), rf_cv.best_estimator_.feature_importances_)\n",
    "plt.xlabel(\"Feature/Gene index\")\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.title(\"Random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider that all features with a non-zero importance are selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rf_cv.best_estimator_.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a lot, so we can also set a threshold by hand (either on the number of features to keep, or on the importance value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here, we can decide to keep only the feature with an importance at least equal to 0.005:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(rf_cv.best_estimator_.feature_importances_ >= 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(rf_cv.best_estimator_.feature_importances_ >= 0.005)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we really use only these features? Let's retrain a random forest only on those features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = np.nonzero(rf_cv.best_estimator_.feature_importances_ >= 0.005)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = X_train[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reduced_cv = model_selection.GridSearchCV(ensemble.RandomForestClassifier(),  \n",
    "                                     {'n_estimators': ntrees_values},\n",
    "                                     cv=5, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reduced_cv.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of trees is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reduced_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal cross-validated balanced accuracy: %.3f\" % rf_reduced_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance didn't really drop that much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = X_test[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = rf_reduced_cv.best_estimator_.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced accuracy of the random forest: %.3f\" % metrics.balanced_accuracy_score(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(rf_reduced_cv.best_estimator_, X_test_reduced, y_test, \n",
    "                             cmap=plt.cm.Blues # use a blue color map\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model makes very few mistakes and uses very few genes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
